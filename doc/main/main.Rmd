---
title: "Investigation of Sparse Hierarchical Regularization for Basis Expansion Methods"
subtitle: "Exploration and Expansion Regression via `HierBasis`"
author: 
  David Fleischer
  Annik Gougeon
date: "Last Update: `r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
bibliography: ../references/references.bib
header-includes:
   - \usepackage{amsmath,amsthm,amssymb,mathtools,bm,algorithm}
   - \usepackage[noend]{algpseudocode}
---

\newcommand{\bbeta}{{\bm \beta}}
\newcommand{\argmin}{{\text{arg min}}}
\newcommand{\X}{{\mathbb X}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\eps}{\varepsilon}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\phi}{\varphi}
\renewcommand{\bbeta}{\bm \beta} 

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

The method of nonparametric regression regularization described in @haris2016nonparametric provides a flexible framework and implementation of a sparse hierarchical penalty via the `R` package `HierBasis`. The proposal offered by @haris2016nonparametric outlines a convex penaltization and estimation technique that is suggested to be well-suited to high-dimensional problems. In particular, we wish to verify and expand upon the `HierBasis` framework in the context of sparse additive modelling, focusing on the problem of prediction of a continuous response and variable selection.

## Problem Description

We restrict the attention of this project to focus on the problem of regression of a continuous response $y = \left[ y_n, ..., y_n \right] \in \R^n$ on a high-dimensional design matrix $\X = \left[ {\bm x}_1, ..., {\bm x}_n \right]^T = \left[ X_1, ..., X_p \right] \in \R^{n\times p}$, such that
\begin{align*}
  {\bm x}_i &= \left[ x_{i1}, ..., x_{ip} \right] \quad \text{(observation $i$)} \\
  X_j &= \left[ x_{1j}, ..., x_{nj} \right]^T \quad \text{(predictor $j$)}.
\end{align*}

We consider the problem of estimating additive components $\left\{f_j\right\}^p_{j = 1}$ of the additive model
$$
  y_i = \sum^p_{j = 1} f_j\left(x_{ij}\right) + \eps_i,
$$

for a sparse set of active features $f_j(x_{ij}) \neq 0$. The proposal offered by @haris2016nonparametric considers the class of basis expansion estimators (@cencov1962estimation) defined by a finite set of basis functions $\left\{ \psi_k(z) \right\}^K_{k = 1}$, with some notion of increasing complexity (in $k$) and for a truncation level $K$ to be adaptively selected. Let $\Psi^{(j)}_K \in \R^{n\times K}$ be the basis expansion corresponding to the $j^\text{th}$ predictor $X_j$, with $(i, k)^{th}$ entry associated with observation $x_{ij}$ and basis function $\psi_k$,
$$
  \Psi^{(j)}_{K,(i, k)} = \psi_k(x_{ij}), \quad 1 \leq k \leq K,~1 \leq i \leq n.
$$
Then, through the basis expansion functions, the design matrix $\X \in \R^{n\times p}$ maps to a set of $p$ $(n\times K)$ matrices 
$$
  \X \stackrel{\psi}{\mapsto} \left\{ \Psi^{(j)}_K \in \R^{n\times K} \right\}^p_{j = 1}.
$$

Of present interest is the set of polynomial basis functions $\left\{ \psi_k(z)\right\}^K_{k = 1} = \left\{ z^k\right\}^K_{k = 1}$ so that
$$
  X_j = 
  \begin{bmatrix}
    x_{1j} \\
    \vdots \\
    x_{nj}
  \end{bmatrix} 
  \mapsto
  \Psi^{(j)}_{K} =
 \begin{bmatrix}
    \psi_1(x_{1j}) & \psi_2(x_{1j}) & \cdots & \psi_K(x_{1j}) \\
    \vdots & \vdots & \ddots & \vdots \\
    \psi_1(x_{nj}) & \psi_2(x_{nj}) & \cdots & \psi_K(x_{nj}) \\
  \end{bmatrix} 
  =
  \begin{bmatrix}
    x_{1j} & x_{1j}^2 & \cdots & x_{1j}^K \\
    \vdots & \vdots & \ddots & \vdots \\
    x_{nj} & x_{nj}^2 & \cdots & x_{nj}^K \\
  \end{bmatrix}.
$$

We estimate the additive components $f_j$ by the \underline{s}parse \underline{a}dditive `HierBasis` estimator $\widehat f_j$ given by
$$
  \widehat f_j(x_{ij}) = \sum^K_{k = 1} \widehat\beta^\texttt{SA-hier}_{j, k} \psi_k(x_{ij}), \quad j = 1, ..., p,
$$

such that the $j = 1, ..., p$ coefficient vectors $\widehat\bbeta^\texttt{SA-hier}_{j} = \left[ \widehat\beta^\texttt{SA-hier}_{j, 1}, ..., \widehat\beta^\texttt{SA-hier}_{j, K} \right] \in \R^K$ are simultaneously estimated by the solution of the minimization problem
\begin{equation}\label{eqn:HierBasis}
 \left[\widehat\bbeta^\texttt{SA-hier}_1, ..., \widehat\bbeta^\texttt{SA-hier}_p \right] = \underset{\bbeta_1, ..., \bbeta_p}{\argmin} \left\{ \frac{1}{2} \left\lVert y - \sum^p_{j = 1} \Psi^{(j)}_K \bbeta_j \right\rVert^2_2 + \lambda \sum^p_{j = 1} \Omega_j\left(\beta_j\right) + \frac{\lambda^2}{\sqrt{n}} \sum^p_{j = 1} \left\lVert \Psi^{(j)}_K \bbeta_j \right\rVert_2 \right\},
\end{equation}

where
$$
\Omega_j(\bbeta_j) = \frac{1}{\sqrt{n}} \sum^K_{k = 1} w_k \left\lVert \Psi^{(j)}_{k:K} \bbeta_{j,k:K} \right\rVert_2,
$$

for weights $w_k = k^m = (k - 1)^m$ penalization weights for the $k^\text{th}$-order basis estimator, $\Psi^{(j)}_{k:K}$ denotes the submatrix of columns $k, k + 1, ..., K$ of $\Psi^{(j)}_K$, and $\bbeta_{j, k:K}$ denotes the corresponding subvector of $\bbeta_j$.

The penalty described in (\ref{eqn:HierBasis}) is defined by two terms. The first term containing $\Omega_j$'s is designed to provide a data-driven method of selecting the basis complexity/truncating the degree of the basis functions to some adaptively selected level $K_0 \leq K$. This term is derived from the hierarchical group lasso penalty (@zhao2009composite) and leads to hierarchical sparsity of the fitted parameters. That is, $\widehat\beta_{j,k} = 0 \implies \widehat\beta_{j, k'} = 0$ for all $k' \geq k$. 

The second term in the sparse additive `HierBasis` penalty $\frac{\lambda^2}{\sqrt{n}} \sum^p_{j = 1} \left\lVert \Psi^{(j)}_K \bbeta_j \right\rVert_2$ imposes sparsity across the predictors $X_1, ..., X_p$ and induces additional sparsity across the solution space $\left\{ \widehat\bbeta^\texttt{SA-hier}_j \right\}^p_{j = 1}$.

## Problem Convexity

It is important to mention the convexity properties contained within this problem. First, we note that the `HierBasis` penalty, $\Omega(\beta)$ is of the hierarchical group lasso form (@zhao2009composite). That is, it belongs to the CAP (Composite Absolute Penalties) family of penalties, which enables the solution to achieve hierarchical sparsity. According to @zhao2009composite, CAP estimators lead to stable estimates along with a more effective use of degrees of freedom. However, they do not generally result in sparser estimates than the lasso (@tibshirani1996regression).

We define the CAP penalty for hierarichal solution. Introduce a node that corresponds to some group of variables, say $G_k$, and let there be a total of $n$ nodes. For every group $G_k$, define $G_{k:n}$ as the groups that should only be added to the model after $G_k$. For example, in a model with both main and interaction effects, the group of interaction effects can only be added after its main effects are included in the model. In the `HierBasis` case, this consists of the higher order terms of the set of basis functions, $\psi_n$. 

Then, a hierarchical sparsity inducing CAP penalty can be defined as
$$T(\beta) = \sum_{i=1}^n \alpha_i \cdot \left\lVert ( \beta_{G_i}, \beta_{G_{i:n}})\right\rVert_{\gamma_i},$$
where $\alpha_m >0, \forall m$ and $1 \leq \gamma_i < \infty$. Note that $\alpha_m$ is a correction factor in the case that a coefficient appears in numerous groups. An important theorem from @zhao2009composite allows us to obtain convexity:

**Theorem** (@zhao2009composite) If $\gamma_i \geq 1, \forall i=1,...n,$ then $T(\beta)$ is convex. Furthermore, if the loss function $L$ is convex in $\beta$, then the objective function of the CAP optimization problem is convex.

It follows that our estimators $\widehat{\beta}^{\texttt{SA-hier}}_1,..., \widehat{\beta}^{\texttt{SA-hier}}_p$ are indeed convex.

## Solving the `HierBasis` Estimators

To solve the sparse additive problem @haris2016nonparametric first solves the equivalent univariate problem by applying the results of @zhao2009composite, @jenatton2010proximal, @jenatton2011proximal. By writing the problem in the form
$$
  \min_{v\in\R^p} \left\{ \left\lVert u - v \right\rVert^2_2 + \lambda \Omega(v) \right\}
$$

where $\Omega$ is a hierarchical penalty of the form described above. We may apply the following proximal gradient descent algorithm (with complexity $O(p)$) (@jenatton2011proximal). Let $\Psi = UV$ such that $U \in \R^{n\times K}$, $U^T U/n = \mathbb I_K$, can be obtained via a QR decomposition on the basis expansion matrix of $\Psi$. Then, the univariate `HierBasis` problem 
$$
  \widehat\bbeta^{\texttt{hier}(K)} = \underset{\bbeta\in\R^K}{\argmin} \left\{ \frac{1}{2} \left\lVert y - \Psi_K \bbeta \right\rVert^2_2 + \frac{\lambda}{\sqrt{n}} \sum^K_{k = 1} \left( k^m - (k - 1)^m \right) \left\lVert \Psi_{k:K} \bbeta_{k:K} \right\rVert_2 \right\}
$$

is solved by reformulating it in a proximal-gradient-descent-friendly format
$$
  \min_{\bbeta\in\R^K} \left\{ \frac{1}{2} \left\lVert U^T y/n - \bbeta \right\rVert^2_2 + \lambda \sum^K_{k = 1} w_k \left\lVert \bbeta_{k:K} \right\rVert_2 \right\}
$$

which itself can be solved via a coordinate descent algorithm (@haris2016nonparametric)
\begin{algorithm}
	\caption{Solving the Univariate \texttt{HierBasis} Problem}\label{euclid}
	\begin{algorithmic}[1]
		\Procedure{HierBasis}{$y, U, \lambda, \left\{w_k \right\}^K_{k = 1}$}
		\State Initialize $\bbeta^{(1)} = \cdots = \bbeta^K \leftarrow U^T y/n$
		\For{k = K, ..., 1}
		\State Update $\bbeta^{k - 1}_{k:K} \leftarrow \left( 1 - \frac{w_k \lambda}{\left\lVert \bbeta^k_{k:K} \right\rVert_2} \right)_+ \bbeta^k_{k:K}$
		\EndFor
		\Return $\bbeta^1$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

With the univariate `HierBasis` estimators solved, we may now introduce the solution to the sparse additive `HierBasis` estimators using a block coordinate descent algorithm (@haris2016nonparametric)
\begin{algorithm}
	\caption{Solving the Sparse Additive \texttt{HierBasis} Problem}\label{euclid}
	\begin{algorithmic}[1]
		\Procedure{AdditiveHierBasis}{$y, \left\{ \Psi^{(j)}_K \right\}^p_{j=1}, \lambda, \left\{ w_k \right\}^K_{k=1}, \texttt{maxiter}$}
		\State Initialize $\beta_j \leftarrow 0$ for $j = 1, ..., p$
		\While{$l \leq \texttt{maxiter}$ {\bf and} \text{not converged}}
		  \For{$j = 1, ..., p$}
		  \State Set
		  $r_{-j} \leftarrow y - \sum_{j'\neq j} \Psi^{(j')}_K \bbeta_{j'}$
		  \State Set
		  $\tilde{w}_1 = w_1 + \lambda,~\tilde{w}_k = w_k,$ for $k = 2, ..., K$
		  \State Update 
		  $\bbeta_j \leftarrow \argmin \left\{ \frac{1}{2n} \left\lVert r_{-j} - \Psi^{(j)}_K \bbeta \right\rVert^2_2 + \frac{\lambda}{\sqrt{n}} \sum^K_{k = 1} \tilde{w}_k \left\lVert \Psi^{(j)}_{k:K} \bbeta_{j, k:K} \right\rVert_2 \right\}$
		  \EndFor
		\EndWhile
		\Return $\bbeta_1, ..., \bbeta_p$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

## Proposal

Of consideration for this project, we wish to tackle the following questions:

(1) Can the \texttt{hierbasis} estimator procedure offer a material gain over the lasso estimator (@tibshirani1996regression)? Preliminary tests, as well as the \texttt{hierbasis} documentation (@hierbasis), suggest a marginal sparsity improvement with no worse predictive power, but at the cost of computational complexity.

(2) The \texttt{hierbasis} documentation (@hierbasis) references a mixing parameter $\alpha$ controlling the relative importance of the hierarchical and the sparsity-inducing penalities. How does the manipulation of this parameter affect its performance? Is it feasible to select $\alpha$ through cross-validation?

(3) What is the effect of changing the form of the weights $w_k = k^m - (k - 1)^m$ in the hierarchical penalty $\Omega$? The documentation suggests implementing $m=2$ or $m=3$. Why are these two values optimal, and how does the procedure perform when another $m$ is selected?

(4) How does the \texttt{hierbasis} estimator procedure and R package perform on new datasets and simulations? Is it feasible to use this method for large datasets, considering the compuation time. How does it compare to the lasso estimator (@tibshirani1996regression) in this regard? 

# Methods

## The `hierbasis2` Package

We have create a companion package to `HierBasis`, named `hierbasis2`, in order to implement the above tests and features of the above proposal. This new package retains all of the user-facing functionality of the original `HierBasis` package, but now permits the user to manipulate some additional parameters, as well as introducing some new functions. That is, the new library has been designed with this project in mind, allowing us to explore the properties of the original `HierBasis` package in a modular, readable, and concise format. 

### Installation

As is the case for `HierBasis`, installation of `hierbasis2` can be done via `devtools::install_github`

```{r, eval = F}
#install.packages(devtools)
library(devtools)
install_github("dfleis/hierbasis2")
library(hierbasis2)
```

# Simulations

```{r, message = F, echo = F}
library(glmnet)
library(hierbasis2)
```

## Comparison with `glmnet`

### Linear Models

We begin with a simple comparison of the `HierBasis` estimators to their lasso counterparts (via `glmnet` @glmnet) in the prediction and variable selection of a simple linear model
$$
y_i = {\bm x}_i \bbeta + \epsilon_i, 
$$

for ${\bm x}_i \sim \mathcal N_p(0, \mathbb I_p)$ and $\epsilon_i \sim \mathcal N(0, \sigma^2)$ such that the noise variance $\sigma^2$ satisfying
$$
  \text{SNR} = \frac{1}{\sigma^2(n - 1)} \sum^n_{i = 1} \left({\bm x}_i \bbeta\right)^2,
$$

for a fixed signal-to-noise ratio $\text{SNR} = 3$. 

```{r}
#===== parameters =====#
set.seed(400)
n <- 1000 # number of observations
p <- 9 # number of predictors (excluding intercept)
SPARSE_PCT <- 0.5 # proportion of sparse predictors
# which predictors are sparse?
SPARSE_IDX <- sample(2:(p + 1), size = floor(SPARSE_PCT * p)) 
SNR <- 3 # signal-to-noise ratio (controls noise dispersions)
beta <- rnorm(p + 1, 0, 10)
beta[SPARSE_IDX] <- 0

#===== generate data =====#
X <- matrix(rnorm(n * p), ncol = p) # iid normal deviates
# compute noiseless response
ytrue <- cbind(1, X) %*% beta
# compute noise dispersion satisfying the SNR ratio
sigma2 <- sum(ytrue^2)/(n - 1) * 1/SNR 
eps <- rnorm(n, 0, sqrt(sigma2))
# compute perturbed response
y <- ytrue + eps

### split data into training and validation sets
X_train <- X[1:(n/2),]; X_valid <- X[(n/2 + 1):n,]
y_train <- y[1:(n/2)]; y_valid <- y[(n/2 + 1):n]
```

#### Prediction Performance

For both the lasso and `HierBasis` estimators we will perform 10-fold cross-validation to select the tuning parameter $\lambda$. For computational considerations we limit the maximum number of basis elements to $K = \texttt{nbasis} = 10$

```{r}
#===== fit models =====#
# lasso
pt <- proc.time()
mod.glmnet.cv <- cv.glmnet(x = X_train, y = y_train, alpha = 1)
proc.time() - pt

# additive hierbasis
pt <- proc.time()
mod.ahb.cv <- cv.additivehierbasis(X = X_train, y = y_train)
proc.time() - pt
```

Below we present the test-error from the cross-validation procedure plotted against the natural logarithm of the tuning parameter, with the number of active features (`glmnet`) and number of active basis elements (`HierBasis`) printed above the figure.
```{r, echo = F, fig.align = 'center', fig.height = 4, fig.width = 5}
plot(mod.glmnet.cv, sub = "glmnet")
plot(mod.ahb.cv, sub = "HierBasis")
```

One empirical observation with the cross-validation procedure is that the method for automatically select the tuning parameters $\lambda$ appears to set the lower bound of the sequence to be too low. If we instead manually set $\lambda$ to be within the range of $[e^{-3}, e^{1.5}]$ we find cross-validation plots that appear to be more reasonable
```{r, echo = F, fig.align = 'center', fig.height = 4, fig.width = 5}
# additive hierbasis
pt <- proc.time()
mod.ahb.cv <- cv.additivehierbasis(X = X_train, y = y_train,
                                   lambdas = exp(seq(1.75, -3.25, length.out = 50)))
proc.time() - pt
plot(mod.ahb.cv, sub = "HierBasis")
```

Using the validation sets we may compare the performance of the regression procedures numerically, using the tuning parameters corresponding to the least testing error, $\lambda_{\text{min}}$, and the one-standard-error rule, $\lambda_{1\text{se}}$.
```{r}
### predict validation sets ###
yhat.glmnet.min <- predict(mod.glmnet.cv, X_valid, s = mod.glmnet.cv$lambda.1se)
yhat.glmnet.1se <- predict(mod.glmnet.cv, X_valid, s = mod.glmnet.cv$lambda.min)
yhat.ahb.min <- predict(mod.ahb.cv, X_valid, lam.idx = mod.ahb.cv$lambda.min.idx)
yhat.ahb.1se <- predict(mod.ahb.cv, X_valid, lam.idx = mod.ahb.cv$lambda.1se.idx)

err.glmnet.min <- yhat.glmnet.min - y_valid
err.glmnet.1se <- yhat.glmnet.1se - y_valid
err.ahb.min <- yhat.ahb.min - y_valid
err.ahb.1se <- yhat.ahb.1se - y_valid

mse.glmnet.min <- mean(err.glmnet.min^2); mse.glmnet.min.sd <- sd(err.glmnet.min)
mse.glmnet.1se <- mean(err.glmnet.1se^2); mse.glmnet.1se.sd <- sd(err.glmnet.1se)
mse.ahb.min <- mean(err.ahb.min^2); mse.ahb.min.sd <- sd(err.ahb.min)
mse.ahb.1se <- mean(err.ahb.1se^2); mse.ahb.1se.sd <- sd(err.ahb.1se)

mse <- cbind(
  c(mse.glmnet.min, mse.glmnet.1se, mse.ahb.min, mse.ahb.1se),
  c(mse.glmnet.min.sd, mse.glmnet.1se.sd, mse.ahb.min.sd, mse.ahb.1se.sd))
colnames(mse) <- c("MSE", "MSE.SD")
rownames(mse) <- c("glmnet.min", "glmnet.1se", "HierBasis.min", "HierBasis.1se")
round(mse, 2)
```

We see that for this regression task the `HierBasis` estimator performs comparably to the lasso, at the cost of added computational (and model) complexity. It should be noted that the linear model is not the scenario the additive `HierBasis` estimator was designed for, and so matching performance with the lasso informs us that the `HierBasis` estimator remains capable of handling simpler response-predictor relationships.

#### Variable Selection

Prior to the comparison between the lasso and `HierBasis` estimators' ability to correctly select active features, we explore the properties of the `HierBasis` estimates. First, we investigate the paths the coefficients take as a function of the tuning parameter $\lambda$. For the simulation above we find the following coefficients (with nonzero values corresponding to the active features of $\X$)
```{r}
round(beta[2:length(beta)], 2) # exclude intercept
```

Plotted below are a set of graphs illustrating the coefficient paths (excluding the intercept), with each predictor presented in a separate plot. Plotted above each figure is the number of active hierarchical basis features for the corresponding value of $\lambda$, with the diffrent lines corresponding to the different hierarchical basis features (with $\lambda_{\text{1se}}$ marked by the vertial dotted line)
```{r, echo = F, fig.align = 'center', fig.height = 3, fig.width = 3.5, fig.show = 'hold'}
for (j in 1:p) {
  plot(mod.ahb.cv$model.fit, 
       pred.idx = j, 
       plot.type = 'lines', 
       plot.stat = 'coef')
  if (beta[j + 1] != 0) {
    legend("topright", legend = "Active", bty = "n")
  }
}
```

We may also visualize the total number of active features the `HierBasis` estimators detect, where a feature is said to be active if all its basis estimates are estimated to zero. Plotted are the number of active features against the tuning parameter, with the covariate index marked on the $y$ axis.
```{r, echo = F, fig.align = 'center', fig.height = 4, fig.width = 4.5}
plot(mod.ahb.cv$model.fit, plot.stat = 'active', legend = T)
```

To compare the lasso and `HierBasis` estimators we display the estimates of $\bbeta$ in both the cases of the one-standard-error rule tuning parameter $\lambda_{\text{1se}}$.
```{r}
betahat.glmnet <- coef(mod.glmnet.cv, s = mod.glmnet.cv$lambda.1se)
betahat.ahb <- coef(mod.ahb.cv, lam.idx = mod.ahb.cv$lambda.1se.idx)

round(beta, 2)
round(as.numeric(betahat.glmnet), 2)
round(betahat.ahb$intercept, 2)
round(betahat.ahb$X[1:3,], 2)
```

Once again, we find `HierBasis` to be comparable to the lasso. In this case both estimates correctly select the active features after the application of the one-standard-error rule.

### Additive Models

The sparse additive `HierBasis` framework was original envisaged to be particularly well-suited to additive modelling. For this reason we now consider the (potentially nonlinear) relationship
$$
  y_i = \sum^p_{j = 1} f_j(x_{ij}) + \epsilon_i,
$$

as defined in the introduction, and with $\epsilon_i$ defined analogously to the linear case (with $\text{SNR} = 3$). For our simulation we consider the additive relationship
$$
  y_i = 2 + 5f_1(x_{i1}) + 3f_2(x_{i2}) + 4f_3(x_{i3}) + 6f_4(x_{i4}) + \epsilon_i,
$$

such that
\begin{align*}
  f_1(x) &= x \\
  f_2(x) &= (2x - 1)^2 \\
  f_3(x) &= \frac{2 \sin(2\pi x)}{2 - \sin(2\pi x)} \\
  f_4(x) &= 0.1 \sin(2\pi x) + 0.2 \cos(2\pi x) + 0.3 \sin^2(2\pi x) + 0.4 \cos^3(2\pi x) + 0.5 \sin^3(2\pi x).
\end{align*}

We generate our features ${\bm x}_i \stackrel{\text{iid}}{\sim} \mathcal N_p(0, 1)$ from i.i.d. standard normal deviates with $n$ and $p$ set immediately below.
```{r}
#===== data parameters =====#
set.seed(680)
n <- 1000 # number of observations
p <- 9 # number of predictors (excluding intercept)
SNR <- 3 # signal to noise ratio
nbasis <- 10

#===== functions =====#
f1 <- function(x) x
f2 <- function(x) (2 * x - 1)^2
f3 <- function(x) 2 * sin(2 * pi * x)/(2 - sin(2 * pi * x))
f4 <- function(x) 0.1 * sin(2 * pi * x) + 0.2 * cos(2 * pi * x) + 
  0.3 * sin(2 * pi * x)^2 + 
  0.4 * cos(2 * pi * x)^3 + 
  0.5 * sin(2 * pi * x)^3

#===== generate data =====#
X <- matrix(rnorm(n * p), ncol = p) # iid normal deviates
y1 <- 5 * f1(X[, 1])
y2 <- 3 * f2(X[, 2])
y3 <- 4 * f3(X[, 3])
y4 <- 6 * f4(X[, 4])
ytrue <- 2 + y1 + y2 + y3 + y4

# compute noise dispersion satisfying the SNR ratio
sigma2 <- sum(ytrue^2)/(n - 1) * 1/SNR 
eps <- rnorm(n, 0, sqrt(sigma2))

# compute perturbed response
y <- ytrue + eps

### split data into training and validation sets ###
X_train <- X[1:(n/2),]
X_valid <- X[(n/2 + 1):n,]
y_train <- y[1:(n/2)]
y_valid <- y[(n/2 + 1):n]
```

#### Prediction

Once again, we set $K = \texttt{nbasis} = 10$ to be the number of basis features to consider, and perform 10-fold cross-validation over the set of tuning parameters $\lambda$.

```{r}
#===== fit models =====#
# lasso
pt <- proc.time()
mod.glmnet.cv <- cv.glmnet(x = X_train, y = y_train, alpha = 1)
proc.time() - pt

# additive hierbasis
pt <- proc.time()
mod.ahb.cv <- cv.additivehierbasis(X = X_train, y = y_train, nbasis = nbasis, 
                                   lambdas = exp(seq(1.5, -3, length.out = 50)))
proc.time() - pt
```

```{r, echo = F, fig.align = 'center', fig.height = 4, fig.width = 5}
plot(mod.glmnet.cv, sub = "glmnet")
plot(mod.ahb.cv, sub = "HierBasis")
```

We again use the validation sets to compare estimator performance under both the least testing error, $\lambda_{\text{min}}$, and the one-standard-error rule, $\lambda_{1\text{se}}$, tuning parameters
```{r}
### predict validation sets ###
yhat.glmnet.min <- predict(mod.glmnet.cv, X_valid, s = mod.glmnet.cv$lambda.1se)
yhat.glmnet.1se <- predict(mod.glmnet.cv, X_valid, s = mod.glmnet.cv$lambda.min)
yhat.ahb.min <- predict(mod.ahb.cv, X_valid, lam.idx = mod.ahb.cv$lambda.min.idx)
yhat.ahb.1se <- predict(mod.ahb.cv, X_valid, lam.idx = mod.ahb.cv$lambda.1se.idx)

err.glmnet.min <- yhat.glmnet.min - y_valid
err.glmnet.1se <- yhat.glmnet.1se - y_valid
err.ahb.min <- yhat.ahb.min - y_valid
err.ahb.1se <- yhat.ahb.1se - y_valid

mse.glmnet.min <- mean(err.glmnet.min^2)
mse.glmnet.min.sd <- sd(err.glmnet.min)
mse.glmnet.1se <- mean(err.glmnet.1se^2)
mse.glmnet.1se.sd <- sd(err.glmnet.1se)
mse.ahb.min <- mean(err.ahb.min^2)
mse.ahb.min.sd <- sd(err.ahb.min)
mse.ahb.1se <- mean(err.ahb.1se^2)
mse.ahb.1se.sd <- sd(err.ahb.1se)

mse <- cbind(
  c(mse.glmnet.min, mse.glmnet.1se, mse.ahb.min, mse.ahb.1se),
  c(mse.glmnet.min.sd, mse.glmnet.1se.sd, mse.ahb.min.sd, mse.ahb.1se.sd))
colnames(mse) <- c("MSE", "MSE.SD")
rownames(mse) <- c("glmnet.min", "glmnet.1se", "HierBasis.min", "HierBasis.1se")
round(mse, 2)
```

Although it may not be immediately obvious from the cross-validaiton figures, the numeric results demonstrate a clear advantage of the `HierBasis` estimator's ability to capture additive relationships in the data (at the cost of computational performance). We may see this comparison more clearly (and rigorously) by performing the above computations a number of times in order to generate a distribution of squared residuals. To this end, we repeat this process $\texttt{nsims} = 100$ times and plot the results below
```{r, echo = F}
nsims <- 100

pt <- proc.time()
sim <- replicate(nsims, {
  #===== generate data =====#
  X <- matrix(rnorm(n * p), ncol = p) # iid normal deviates
  y1 <- 5 * f1(X[, 1])
  y2 <- 3 * f2(X[, 2])
  y3 <- 4 * f3(X[, 3])
  y4 <- 6 * f4(X[, 4])
  ytrue <- 2 + y1 + y2 + y3 + y4
  
  # compute noise dispersion satisfying the SNR ratio
  sigma2 <- sum(ytrue^2)/(n - 1) * 1/SNR 
  eps <- rnorm(n, 0, sqrt(sigma2))
  
  # compute perturbed response
  y <- ytrue + eps
  
  ### split data into training and validation sets ###
  X_train <- X[1:(n/2),]
  X_valid <- X[(n/2 + 1):n,]
  y_train <- y[1:(n/2)]
  y_valid <- y[(n/2 + 1):n]
  
  #===== fit models =====#
  # lasso
  mod.glmnet.cv <- cv.glmnet(x = X_train, y = y_train, alpha = 1)
  # additive hierbasis
  mod.ahb.cv <- cv.additivehierbasis(X = X_train, y = y_train, 
                                     nbasis = nbasis,
                                     lambdas = exp(seq(1.25, -3, 
                                                       length.out = 50)))
  
  # predict
  yhat.glmnet.min <- predict(mod.glmnet.cv, X_valid, 
                             s = mod.glmnet.cv$lambda.1se)
  yhat.glmnet.1se <- predict(mod.glmnet.cv, X_valid, 
                             s = mod.glmnet.cv$lambda.min)
  yhat.ahb.min <- predict(mod.ahb.cv, X_valid, 
                          lam.idx = mod.ahb.cv$lambda.min.idx)
  yhat.ahb.1se <- predict(mod.ahb.cv, X_valid, 
                          lam.idx = mod.ahb.cv$lambda.1se.idx)
  
  # compute errors
  err.glmnet.min <- yhat.glmnet.min - y_valid
  err.glmnet.1se <- yhat.glmnet.1se - y_valid
  err.ahb.min <- yhat.ahb.min - y_valid
  err.ahb.1se <- yhat.ahb.1se - y_valid
  
  list("glmnet.min" = err.glmnet.min,
       "glmnet.1se" = err.glmnet.1se,
       "ahb.min" = err.ahb.min,
       "ahb.1se" = err.ahb.1se)
})
proc.time() - pt
```

```{r, echo = F, fig.height = 4, fig.width = 5, fig.align = 'center'}
glmnet.min <- sapply(sim["glmnet.min",], function(x) mean(x^2))
glmnet.1se <- sapply(sim["glmnet.1se",], function(x) mean(x^2))
ahb.min <- sapply(sim["ahb.min",], function(x) mean(x^2))
ahb.1se <- sapply(sim["ahb.1se",], function(x) mean(x^2))

d.glmnet.min <- density(glmnet.min)
d.glmnet.1se <- density(glmnet.1se)
d.ahb.min <- density(ahb.min)
d.ahb.1se <- density(ahb.1se)

plot(d.glmnet.min, lwd = 2, lty = 'dashed', col = 'gray60',
     xlim = range(d.glmnet.min$x, d.glmnet.1se$x,
                  d.ahb.min$x, d.ahb.1se$x),
     ylim = range(d.glmnet.min$y, d.glmnet.1se$y,
                  d.ahb.min$y, d.ahb.1se$y),
     xlab = "Mean-Square Error", main = "MSE Density")
lines(d.glmnet.1se, lwd = 2, lty = 'dotted', col = 'gray60')
lines(d.ahb.min, lwd = 2, lty = 'dashed')
lines(d.ahb.1se, lwd = 2, lty = 'dotted')
legend("topright", legend = c("gnet.min", "gnet.1se", 
                              "HB.min", "HB.1se"),
       lty = c("dashed", "dotted", "dashed", "dotted"),
       col = c("gray60", "gray60", "black", "black"),
       bty = "n", lwd = 2)
```


#### Variable Selection

We repeat the figures from the linear model variable selection section for the additive modelling environment. First, we investigate `HierBasis`'s ability to select the correct features from the design matrix via the coefficient plots as a function of $\lambda$ (with $\lambda_{\text{1se}}$ marked by the vertial dotted line)
```{r, echo = F, fig.align = 'center', fig.height = 3, fig.width = 3.5, fig.show = 'hold'}
for (j in 1:p) {
  plot(mod.ahb.cv$model.fit, 
       pred.idx = j, 
       plot.type = 'lines', 
       plot.stat = 'coef')
  if (j <= 5) {
    legend("topright", legend = "Active", bty = "n")
  }
}
```

## Manipulating the Mixing Parameter $\alpha$

## Manipulating Penalty Weights $w_k$

### Polynomial Growth: Manipulating $m$

### Exponential Growth

### Logarithmic Growth


\newpage
# References



