---
title: "Investigation of Sparse Hierarchical Regularization for Basis Expansion Methods"
subtitle: "Exploration and Expansion Regression via `HierBasis`"
author: 
  David Fleischer
  Annik Gougeon
date: "Last Update: `r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
bibliography: ../references/references.bib
header-includes:
   - \usepackage{amsmath,amsthm,amssymb,mathtools,bm,algorithm}
   - \usepackage[noend]{algpseudocode}
---

\newcommand{\bbeta}{{\bm \beta}}
\newcommand{\argmin}{{\text{arg min}}}
\newcommand{\X}{{\mathbb X}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\eps}{\varepsilon}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\phi}{\varphi}
\renewcommand{\bbeta}{\bm \beta} 

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

The method of nonparametric regression regularization described in @haris2016nonparametric provides a flexible framework and implementation of a sparse hierarchical penalty via the `R` package `HierBasis`. The proposal offered by @haris2016nonparametric outlines a convex penaltization and estimation technique that is suggested to be well-suited to high-dimensional problems. In particular, we wish to verify and expand upon the `HierBasis` framework in the context of sparse additive modelling, focusing on the problem of prediction of a continuous response and variable selection.

## Problem Description

We restrict the attention of this project to focus on the problem of regression of a continuous response $y = \left[ y_n, ..., y_n \right] \in \R^n$ on a high-dimensional design matrix $\X = \left[ {\bm x}_1, ..., {\bm x}_n \right]^T = \left[ X_1, ..., X_p \right] \in \R^{n\times p}$, such that
\begin{align*}
  {\bm x}_i &= \left[ x_{i1}, ..., x_{ip} \right] \quad \text{(observation $i$)} \\
  X_j &= \left[ x_{1j}, ..., x_{nj} \right]^T \quad \text{(predictor $j$)}.
\end{align*}

We consider the problem of estimating additive components $\left\{f_j\right\}^p_{j = 1}$ of the additive model
$$
  y_i = \sum^p_{j = 1} f_j\left(x_{ij}\right) + \eps_i,
$$

for a sparse set of active features $f_j(x_{ij}) \neq 0$. The proposal offered by @haris2016nonparametric considers the class of basis expansion estimators (@cencov1962estimation) defined by a finite set of basis functions $\left\{ \psi_k(z) \right\}^K_{k = 1}$, with some notion of increasing complexity (in $k$) and for a truncation level $K$ to be adaptively selected. Let $\Psi^{(j)}_K \in \R^{n\times K}$ be the basis expansion corresponding to the $j^\text{th}$ predictor $X_j$, with $(i, k)^{th}$ entry associated with observation $x_{ij}$ and basis function $\psi_k$,
$$
  \Psi^{(j)}_{K,(i, k)} = \psi_k(x_{ij}), \quad 1 \leq k \leq K,~1 \leq i \leq n.
$$
Then, through the basis expansion functions, the design matrix $\X \in \R^{n\times p}$ maps to a set of $p$ $(n\times K)$ matrices 
$$
  \X \stackrel{\psi}{\mapsto} \left\{ \Psi^{(j)}_K \in \R^{n\times K} \right\}^p_{j = 1}.
$$

Of present interest is the set of polynomial basis functions $\left\{ \psi_k(z)\right\}^K_{k = 1} = \left\{ z^k\right\}^K_{k = 1}$ so that
$$
  X_j = 
  \begin{bmatrix}
    x_{1j} \\
    \vdots \\
    x_{nj}
  \end{bmatrix} 
  \mapsto
  \Psi^{(j)}_{K} =
 \begin{bmatrix}
    \psi_1(x_{1j}) & \psi_2(x_{1j}) & \cdots & \psi_K(x_{1j}) \\
    \vdots & \vdots & \ddots & \vdots \\
    \psi_1(x_{nj}) & \psi_2(x_{nj}) & \cdots & \psi_K(x_{nj}) \\
  \end{bmatrix} 
  =
  \begin{bmatrix}
    x_{1j} & x_{1j}^2 & \cdots & x_{1j}^K \\
    \vdots & \vdots & \ddots & \vdots \\
    x_{nj} & x_{nj}^2 & \cdots & x_{nj}^K \\
  \end{bmatrix}.
$$

We estimate the additive components $f_j$ by the \underline{s}parse \underline{a}dditive `HierBasis` estimator $\widehat f_j$ given by
$$
  \widehat f_j(x_{ij}) = \sum^K_{k = 1} \widehat\beta^\texttt{SA-hier}_{j, k} \psi_k(x_{ij}), \quad j = 1, ..., p,
$$

such that the $j = 1, ..., p$ coefficient vectors $\widehat\bbeta^\texttt{SA-hier}_{j} = \left[ \widehat\beta^\texttt{SA-hier}_{j, 1}, ..., \widehat\beta^\texttt{SA-hier}_{j, K} \right] \in \R^K$ are simultaneously estimated by the solution of the minimization problem
\begin{equation}\label{eqn:HierBasis}
 \left[\widehat\bbeta^\texttt{SA-hier}_1, ..., \widehat\bbeta^\texttt{SA-hier}_p \right] = \underset{\bbeta_1, ..., \bbeta_p}{\argmin} \left\{ \frac{1}{2} \left\lVert yg - \sum^p_{j = 1} \Psi^{(j)}_K \bbeta_j \right\rVert^2_2 + \lambda \sum^p_{j = 1} \Omega_j\left(\beta_j\right) + \frac{\lambda^2}{\sqrt{n}} \sum^p_{j = 1} \left\lVert \Psi^{(j)}_K \bbeta_j \right\rVert_2 \right\},
\end{equation}

where
$$
\Omega_j(\bbeta_j) = \frac{1}{\sqrt{n}} \sum^K_{k = 1} w_k \left\lVert \Psi^{(j)}_{k:K} \bbeta_{j,k:K} \right\rVert_2,
$$

for weights $w_k = k^m = (k - 1)^m$ penalization weights for the $k^\text{th}$-order basis estimator, $\Psi^{(j)}_{k:K}$ denotes the submatrix of columns $k, k + 1, ..., K$ of $\Psi^{(j)}_K$, and $\bbeta_{j, k:K}$ denotes the corresponding subvector of $\bbeta_j$.

The penalty described in (\ref{eqn:HierBasis}) is defined by two terms. The first term containing $\Omega_j$'s is designed to provide a data-driven method of selecting the basis complexity/truncating the degree of the basis functions to some adaptively selected level $K_0 \leq K$. This term is derived from the hierarchical group lasso penalty (@zhao2009composite) and leads to hierarchical sparsity of the fitted parameters. That is, $\widehat\beta_{j,k} = 0 \implies \widehat\beta_{j, k'} = 0$ for all $k' \geq k$. 

The second term in the sparse additive `HierBasis` penalty $\frac{\lambda^2}{\sqrt{n}} \sum^p_{j = 1} \left\lVert \Psi^{(j)}_K \bbeta_j \right\rVert_2$ imposes sparsity across the predictors $X_1, ..., X_p$ and induces additional sparsity across the solution space $\left\{ \widehat\bbeta^\texttt{SA-hier}_j \right\}^p_{j = 1}$.

## Problem Convexity

It is important to mention the convexity properties contained within this problem. First, we note that the `HierBasis` penalty, $\Omega(\beta)$ is of the hierarchical group lasso form (@zhao2009composite). That is, it belongs to the CAP (Composite Absolute Penalties) family of penalties, which enables the solution to achieve hierarchical sparsity. According to @zhao2009composite, CAP estimators lead to stable estimates along with a more effective use of degrees of freedom. However, they do not generally result in sparser estimates than the lasso (@tibshirani1996regression).

We define the CAP penalty for hierarichal solution. Introduce a node that corresponds to some group of variables, say $G_k$, and let there be a total of $n$ nodes. For every group $G_k$, define $G_{k:n}$ as the groups that should only be added to the model after $G_k$. For example, in a model with both main and interaction effects, the group of interaction effects can only be added after its main effects are included in the model. In the `HierBasis` case, this consists of the higher order terms of the set of basis functions, $\psi_n$. 

Then, a hierarchical sparsity inducing CAP penalty can be defined as
$$T(\beta) = \sum_{i=1}^n \alpha_i \cdot \left\lVert ( \beta_{G_i}, \beta_{G_{i:n}})\right\rVert_{\gamma_i},$$
where $\alpha_m >0, \forall m$ and $1 \leq \gamma_i < \infty$. Note that $\alpha_m$ is a correction factor in the case that a coefficient appears in numerous groups. An important theorem from @zhao2009composite allows us to obtain convexity:

**Theorem** (@zhao2009composite) If $\gamma_i \geq 1, \forall i=1,...n,$ then $T(\beta)$ is convex. Furthermore, if the loss function $L$ is convex in $\beta$, then the objective function of the CAP optimization problem is convex.

It follows that our estimators $\widehat{\beta}^{\texttt{SA-hier}}_1,..., \widehat{\beta}^{\texttt{SA-hier}}_p$ are indeed convex.

## Solving the `HierBasis` Estimators

To solve the sparse additive problem @haris2016nonparametric first solves the equivalent univariate problem by applying the results of @zhao2009composite, @jenatton2010proximal, @jenatton2011proximal. By writing the problem in the form
$$
  \min_{v\in\R^p} \left\{ \left\lVert u - v \right\rVert^2_2 + \lambda \Omega(v) \right\}
$$

where $\Omega$ is a hierarchical penalty of the form described above. We may apply the following proximal gradient descent algorithm (with complexity $O(p)$) (@jenatton2011proximal). Let $\Psi = UV$ such that $U \in \R^{n\times K}$, $U^T U/n = \mathbb I_K$, can be obtained via a QR decomposition on the basis expansion matrix of $\Psi$. Then, the univariate `HierBasis` problem 
$$
  \widehat\bbeta^{\texttt{hier}(K)} = \underset{\bbeta\in\R^K}{\argmin} \left\{ \frac{1}{2} \left\lVert y - \Psi_K \bbeta \right\rVert^2_2 + \frac{\lambda}{\sqrt{n}} \sum^K_{k = 1} \left( k^m - (k - 1)^m \right) \left\lVert \Psi_{k:K} \bbeta_{k:K} \right\rVert_2 \right\}
$$

is solved by reformulating it in a proximal-gradient-descent-friendly format
$$
  \min_{\bbeta\in\R^K} \left\{ \frac{1}{2} \left\lVert U^T y/n - \bbeta \right\rVert^2_2 + \lambda \sum^K_{k = 1} w_k \left\lVert \bbeta_{k:K} \right\rVert_2 \right\}
$$

which itself can be solved via a coordinate descent algorithm (@haris2016nonparametric)
\begin{algorithm}
	\caption{Solving the Univariate \texttt{HierBasis} Problem}\label{euclid}
	\begin{algorithmic}[1]
		\Procedure{HierBasis}{$y, U, \lambda, \left\{w_k \right\}^K_{k = 1}$}
		\State Initialize $\bbeta^{(1)} = \cdots = \bbeta^K \leftarrow U^T y/n$
		\For{k = K, ..., 1}
		\State Update $\bbeta^{k - 1}_{k:K} \leftarrow \left( 1 - \frac{w_k \lambda}{\left\lVert \bbeta^k_{k:K} \right\rVert_2} \right)_+ \bbeta^k_{k:K}$
		\EndFor
		\Return $\bbeta^1$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

With the univariate `HierBasis` estimators solved, we may now introduce the solution to the sparse additive `HierBasis` estimators using a block coordinate descent algorithm (@haris2016nonparametric)
\begin{algorithm}
	\caption{Solving the Sparse Additive \texttt{HierBasis} Problem}\label{euclid}
	\begin{algorithmic}[1]
		\Procedure{AdditiveHierBasis}{$y, \left\{ \Psi^{(j)}_K \right\}^p_{j=1}, \lambda, \left\{ w_k \right\}^K_{k=1}, \texttt{maxiter}$}
		\State Initialize $\beta_j \leftarrow 0$ for $j = 1, ..., p$
		\While{$l \leq \texttt{maxiter}$ {\bf and} \text{not converged}}
		  \For{$j = 1, ..., p$}
		  \State Set
		  $r_{-j} \leftarrow y - \sum_{j'\neq j} \Psi^{(j')}_K \bbeta_{j'}$
		  \State Set
		  $\tilde{w}_1 = w_1 + \lambda,~\tilde{w}_k = w_k,$ for $k = 2, ..., K$
		  \State Update 
		  $\bbeta_j \leftarrow \argmin \left\{ \frac{1}{2n} \left\lVert r_{-j} - \Psi^{(j)}_K \bbeta \right\rVert^2_2 + \frac{\lambda}{\sqrt{n}} \sum^K_{k = 1} \tilde{w}_k \left\lVert \Psi^{(j)}_{k:K} \bbeta_{j, k:K} \right\rVert_2 \right\}$
		  \EndFor
		\EndWhile
		\Return $\bbeta_1, ..., \bbeta_p$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

## Proposal

Of consideration for this project, we wish to tackle the following questions:

(1) Can the \texttt{hierbasis} estimator procedure offer a material gain over the lasso estimator (@tibshirani1996regression)? Preliminary tests, as well as the \texttt{hierbasis} documentation (@hierbasis), suggest a marginal sparsity improvement with no worse predictive power, but at the cost of computational complexity.

(2) The \texttt{hierbasis} documentation (@hierbasis) references a mixing parameter $\alpha$ controlling the relative importance of the hierarchical and the sparsity-inducing penalities. How does the manipulation of this parameter affect its performance? Is it feasible to select $\alpha$ through cross-validation?

(3) What is the effect of changing the form of the weights $w_k = k^m - (k - 1)^m$ in the hierarchical penalty $\Omega$? The documentation suggests implementing $m=2$ or $m=3$. Why are these two values optimal, and how does the procedure perform when another $m$ is selected?

(4) How does the \texttt{hierbasis} estimator procedure and R package perform on new datasets and simulations? Is it feasible to use this method for large datasets, considering the compuation time. How does it compare to the lasso estimator (@tibshirani1996regression) in this regard? 

# Methods

## The `hierbasis2` Package

We have create a companion package to `HierBasis`, named `hierbasis2`, in order to implement the above tests and features of the above proposal. This new package retains all of the user-facing functionality of the original `HierBasis` package, but now permits the user to manipulate some additional parameters, as well as introducing some new functions. That is, the new library has been designed with this project in mind, allowing us to explore the properties of the original `HierBasis` package in a modular, readable, and concise format. 

### Installation

As is the case for `HierBasis`, installation of `hierbasis2` can be done via `devtools::install_github`

```{r, eval = F}
#install.packages(devtools)
library(devtools)
install_github("dfleis/hierbasis2")
library(hierbasis2)
```

# Simulations

```{r, message = F, echo = F}
library(glmnet)
library(hierbasis2)
```

## Comparison with `glmnet`

## Manipulating the Mixing Parameter $\alpha$

## Manipulating Penalty Weights $w_k$

## Large Data 

### $n >> p$

### $p >> n$



\newpage
# References



